package io.javafleet.fleetnavigator.experts.runtime;

import io.javafleet.fleetnavigator.config.FleetPathsConfiguration;
import io.javafleet.fleetnavigator.config.LLMConfigProperties;
import io.javafleet.fleetnavigator.experts.model.Expert;
import io.javafleet.fleetnavigator.experts.model.ExpertMode;
import io.javafleet.fleetnavigator.experts.repository.ExpertRepository;
import io.javafleet.fleetnavigator.llm.LLMProvider;
import io.javafleet.fleetnavigator.llm.providers.ExternalLlamaServerProvider;
import io.javafleet.fleetnavigator.llm.providers.JavaLlamaCppProvider;
import io.javafleet.fleetnavigator.llm.providers.OllamaProvider;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.ConcurrentHashMap;

/**
 * Factory f√ºr ExpertRuntime-Instanzen.
 *
 * Verantwortlich f√ºr:
 * - Aufl√∂sung des richtigen LLM-Providers
 * - Aufl√∂sung des Modell-Pfades (GGUF)
 * - Caching von ExpertRuntime-Instanzen
 *
 * @author JavaFleet Systems Consulting
 * @since 0.5.1
 */
@Service
@Slf4j
@RequiredArgsConstructor
public class ExpertRuntimeFactory {

    private final ExpertRepository expertRepository;
    private final FleetPathsConfiguration pathsConfig;
    private final LLMConfigProperties llmConfig;
    private final JavaLlamaCppProvider javaLlamaCppProvider;
    private final OllamaProvider ollamaProvider;
    private final ExternalLlamaServerProvider llamaServerProvider;

    // Cache f√ºr ExpertRuntime (Key: expertId_modeId_cpuOnly)
    private final Map<String, ExpertRuntime> runtimeCache = new ConcurrentHashMap<>();

    /**
     * Erstellt oder holt gecachte ExpertRuntime f√ºr einen Experten.
     *
     * @param expertId ID des Experten
     * @param modeId ID des aktiven Modus (kann null sein)
     * @param cpuOnly CPU-Only Modus
     * @return ExpertRuntime oder empty wenn Experte nicht gefunden
     */
    public Optional<ExpertRuntime> getRuntime(Long expertId, Long modeId, Boolean cpuOnly) {
        if (expertId == null) {
            return Optional.empty();
        }

        String cacheKey = buildCacheKey(expertId, modeId, cpuOnly);

        // Cache pr√ºfen
        ExpertRuntime cached = runtimeCache.get(cacheKey);
        if (cached != null) {
            log.debug("üéì ExpertRuntime aus Cache: {}", cached.getName());
            return Optional.of(cached);
        }

        // Expert laden
        Optional<Expert> expertOpt = expertRepository.findById(expertId);
        if (expertOpt.isEmpty()) {
            log.warn("Expert mit ID {} nicht gefunden", expertId);
            return Optional.empty();
        }

        Expert expert = expertOpt.get();

        // Aktiven Modus finden
        ExpertMode activeMode = null;
        if (modeId != null) {
            activeMode = expert.getModes().stream()
                .filter(m -> modeId.equals(m.getId()))
                .findFirst()
                .orElse(null);
        }

        // Provider ausw√§hlen
        LLMProvider provider = selectProvider(expert);

        // Modell-Pfad aufl√∂sen
        Path resolvedPath = resolveModelPath(expert, provider);

        // ExpertRuntime erstellen
        ExpertRuntime runtime = new ExpertRuntime(
            expert, activeMode, provider, resolvedPath, cpuOnly
        );

        // In Cache speichern
        runtimeCache.put(cacheKey, runtime);

        return Optional.of(runtime);
    }

    /**
     * Erstellt ExpertRuntime ohne Caching (f√ºr einmalige Verwendung)
     */
    public Optional<ExpertRuntime> createRuntime(Expert expert, ExpertMode activeMode, Boolean cpuOnly) {
        if (expert == null) {
            return Optional.empty();
        }

        LLMProvider provider = selectProvider(expert);
        Path resolvedPath = resolveModelPath(expert, provider);

        return Optional.of(new ExpertRuntime(expert, activeMode, provider, resolvedPath, cpuOnly));
    }

    /**
     * Cache leeren (z.B. nach Expert-Update)
     */
    public void clearCache() {
        runtimeCache.clear();
        log.info("ExpertRuntime Cache geleert");
    }

    /**
     * Cache f√ºr bestimmten Experten leeren
     */
    public void clearCacheForExpert(Long expertId) {
        runtimeCache.keySet().removeIf(key -> key.startsWith(expertId + "_"));
        log.debug("Cache f√ºr Expert {} geleert", expertId);
    }

    // ===== Private Hilfsmethoden =====

    private String buildCacheKey(Long expertId, Long modeId, Boolean cpuOnly) {
        return String.format("%d_%s_%s",
            expertId,
            modeId != null ? modeId : "default",
            cpuOnly != null && cpuOnly ? "cpu" : "gpu"
        );
    }

    /**
     * W√§hlt den passenden Provider basierend auf Expert-Konfiguration
     *
     * Priorit√§t:
     * 1. Explizit im Expert gespeicherter providerType
     * 2. GGUF-Modell gesetzt ‚Üí java-llama-cpp oder llama-server
     * 3. baseModel endet auf .gguf ‚Üí java-llama-cpp oder llama-server
     * 4. Default Provider aus Config
     * 5. Fallback auf ersten verf√ºgbaren
     */
    private LLMProvider selectProvider(Expert expert) {
        // 1. PRIORIT√ÑT: Expliziter Provider im Expert
        String providerType = expert.getProviderType();
        if (providerType != null && !providerType.isBlank()) {
            LLMProvider explicit = getProviderByType(providerType);
            if (explicit != null && explicit.isAvailable()) {
                log.debug("üéì Provider f√ºr {}: {} (explizit gespeichert)", expert.getName(), providerType);
                return explicit;
            }
            log.warn("üéì Expliziter Provider '{}' f√ºr {} nicht verf√ºgbar, verwende Fallback",
                providerType, expert.getName());
        }

        // 2. Hat der Expert ein explizites GGUF-Modell? ‚Üí llama-server (bevorzugt) oder java-llama-cpp
        if (expert.getGgufModel() != null && !expert.getGgufModel().isBlank()) {
            if (llamaServerProvider.isAvailable()) {
                log.debug("üéì Provider f√ºr {}: llama-server (GGUF explizit)", expert.getName());
                return llamaServerProvider;
            }
            if (javaLlamaCppProvider.isAvailable()) {
                log.debug("üéì Provider f√ºr {}: java-llama-cpp (GGUF explizit)", expert.getName());
                return javaLlamaCppProvider;
            }
        }

        // 3. Pr√ºfe ob das baseModel ein GGUF-Dateiname ist
        String baseModel = expert.getBaseModel();
        if (baseModel != null && baseModel.toLowerCase().endsWith(".gguf")) {
            if (llamaServerProvider.isAvailable()) {
                log.debug("üéì Provider f√ºr {}: llama-server (baseModel ist GGUF)", expert.getName());
                return llamaServerProvider;
            }
            if (javaLlamaCppProvider.isAvailable()) {
                log.debug("üéì Provider f√ºr {}: java-llama-cpp (baseModel ist GGUF)", expert.getName());
                return javaLlamaCppProvider;
            }
        }

        // 4. Default Provider aus Config
        String defaultProvider = llmConfig.getDefaultProvider();
        LLMProvider defaultProv = getProviderByType(defaultProvider);
        if (defaultProv != null && defaultProv.isAvailable()) {
            log.debug("üéì Provider f√ºr {}: {} (Default)", expert.getName(), defaultProvider);
            return defaultProv;
        }

        // 5. Fallback: Ersten verf√ºgbaren Provider nehmen
        if (llamaServerProvider.isAvailable()) {
            log.debug("üéì Provider f√ºr {}: llama-server (Fallback)", expert.getName());
            return llamaServerProvider;
        }
        if (javaLlamaCppProvider.isAvailable()) {
            log.debug("üéì Provider f√ºr {}: java-llama-cpp (Fallback)", expert.getName());
            return javaLlamaCppProvider;
        }

        log.debug("üéì Provider f√ºr {}: ollama (Fallback)", expert.getName());
        return ollamaProvider;
    }

    /**
     * Gibt Provider anhand des Typ-Strings zur√ºck
     */
    private LLMProvider getProviderByType(String providerType) {
        if (providerType == null) return null;

        return switch (providerType.toLowerCase()) {
            case "llama-server" -> llamaServerProvider;
            case "java-llama-cpp" -> javaLlamaCppProvider;
            case "ollama" -> ollamaProvider;
            default -> null;
        };
    }

    /**
     * L√∂st den Modell-Pfad auf (f√ºr GGUF-Modelle)
     */
    private Path resolveModelPath(Expert expert, LLMProvider provider) {
        // F√ºr java-llama-cpp und llama-server relevant
        String providerName = provider.getProviderName();
        if (!"java-llama-cpp".equals(providerName) && !"llama-server".equals(providerName)) {
            return null;
        }

        Path modelsDir = pathsConfig.getResolvedModelsDir();
        String modelName = expert.getGgufModel();

        // Fallback auf baseModel wenn kein ggufModel gesetzt
        if (modelName == null || modelName.isBlank()) {
            modelName = expert.getBaseModel();
        }

        if (modelName == null || modelName.isBlank()) {
            log.warn("Kein Modell f√ºr Expert {} konfiguriert", expert.getName());
            return null;
        }

        // Verschiedene Pfade versuchen
        Path[] candidates = {
            modelsDir.resolve(modelName),
            modelsDir.resolve("library").resolve(modelName),
            modelsDir.resolve("custom").resolve(modelName)
        };

        for (Path candidate : candidates) {
            if (Files.exists(candidate)) {
                try {
                    // Symlinks aufl√∂sen f√ºr native Bibliothek (llama.cpp)
                    Path realPath = candidate.toRealPath();
                    log.info("‚úÖ Modell gefunden f√ºr {}: {} ‚Üí {}", expert.getName(), candidate, realPath);
                    return realPath;
                } catch (IOException e) {
                    log.warn("‚ö†Ô∏è Konnte Symlink nicht aufl√∂sen f√ºr {}: {}", candidate, e.getMessage());
                    return candidate;
                }
            }
        }

        // Letzte Chance: Suche mit Teil-Match
        Path found = searchModelFile(modelsDir, modelName);
        if (found != null) {
            try {
                Path realPath = found.toRealPath();
                log.info("‚úÖ Modell gefunden (Fuzzy) f√ºr {}: {} ‚Üí {}", expert.getName(), found, realPath);
                return realPath;
            } catch (IOException e) {
                log.info("‚úÖ Modell gefunden (Fuzzy) f√ºr {}: {}", expert.getName(), found);
                return found;
            }
        }

        log.warn("‚ö†Ô∏è Modell nicht gefunden f√ºr {}: {} in {}", expert.getName(), modelName, modelsDir);
        return null;
    }

    /**
     * Sucht nach einem Modell mit Teil-Match
     */
    private Path searchModelFile(Path modelsDir, String modelName) {
        try {
            // Extrahiere Basis-Namen ohne Pfad
            String baseName = modelName;
            if (baseName.contains("/")) {
                baseName = baseName.substring(baseName.lastIndexOf("/") + 1);
            }

            String searchName = baseName.toLowerCase();

            // Suche in library und custom
            for (String subdir : new String[]{"library", "custom", ""}) {
                Path searchDir = subdir.isEmpty() ? modelsDir : modelsDir.resolve(subdir);
                if (!Files.exists(searchDir)) continue;

                var found = Files.list(searchDir)
                    .filter(Files::isRegularFile)
                    .filter(p -> p.toString().toLowerCase().endsWith(".gguf"))
                    .filter(p -> p.getFileName().toString().toLowerCase().contains(
                        searchName.replace(".gguf", "").toLowerCase()))
                    .findFirst();

                if (found.isPresent()) {
                    return found.get();
                }
            }
        } catch (Exception e) {
            log.warn("Fehler bei Modell-Suche: {}", e.getMessage());
        }

        return null;
    }
}
