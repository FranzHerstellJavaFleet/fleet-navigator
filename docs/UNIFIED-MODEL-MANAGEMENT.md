# Unified Model Management - Implementierung

## âœ¨ Was wurde umgesetzt?

Eine **zentrale, provider-abhÃ¤ngige Modellverwaltung** im Model Manager!

### ğŸ¯ Konzept:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ“Š Model Manager (Zentral)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  Tab 1: Installierte Modelle                â”‚
â”‚  â†’ Zeigt alle installierten Modelle         â”‚
â”‚                                              â”‚
â”‚  Tab 2: VerfÃ¼gbare Modelle â† PROVIDER!      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Provider = Ollama                      â”‚ â”‚
â”‚  â”‚ â†’ Zeigt Ollama Library                 â”‚ â”‚
â”‚  â”‚ â†’ Pull-Funktion                        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Provider = llama.cpp                   â”‚ â”‚
â”‚  â”‚ â†’ Zeigt Model Store (9 deutsche GGUFs)â”‚ â”‚
â”‚  â”‚ â†’ HuggingFace Download mit Modal      â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Provider = OpenAI / Andere             â”‚ â”‚
â”‚  â”‚ â†’ Leere Liste (keine Library)         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Implementierte Ã„nderungen

### Backend (bereits vorhanden):
âœ… `/api/llm/providers/active` - Gibt aktiven Provider zurÃ¼ck
âœ… `/api/model-store/*` - Model Store API fÃ¼r llama.cpp

### Frontend:

#### 1. ModelManager.vue erweitert:
- âœ… **Provider Detection** beim Laden
- âœ… **Dynamisches Laden** der Modelle basierend auf Provider:
  - Ollama â†’ `api.getOllamaLibraryModels()`
  - llama.cpp â†’ `/api/model-store/all`
  - Andere â†’ Leere Liste
- âœ… **Provider-spezifische Anzeige**:
  - Ollama: Klassische ModellgrÃ¶ÃŸe + Datum
  - llama.cpp: Beschreibung, Rating, Downloads
- âœ… **Download-FunktionalitÃ¤t**:
  - Ollama: Bestehender Dialog
  - llama.cpp: Neues groÃŸes Modal mit Progress

#### 2. SettingsModal.vue bereinigt:
- âœ… **Model Store Tab entfernt**
- âœ… Imports bereinigt
- âœ… Tab-Liste aktualisiert

---

## ğŸ“Š Provider-UnterstÃ¼tzung

| Provider | VerfÃ¼gbare Modelle | Download | FunktionalitÃ¤t |
|----------|-------------------|----------|----------------|
| **Ollama** | ~250 kuratiert | Pull via Ollama | Bestehender Dialog |
| **llama.cpp** | 9 deutsche GGUFs | HuggingFace Download | GroÃŸes Modal mit Progress |
| **OpenAI** | - | - | Nur API Keys |
| **Weitere** | - | - | Erweiterbar |

---

## ğŸª Model Store fÃ¼r llama.cpp

### VerfÃ¼gbare Modelle:
1. **Qwen 2.5 (3B)** - 1.97 GB - â­ Empfohlen fÃ¼r Deutsch
2. **Llama 3.2 (3B)** - 2.02 GB - Schnell & gut
3. **Qwen 2.5 (7B)** - 4.73 GB - Premium-QualitÃ¤t
4. **Qwen 2.5 Coder (3B/7B)** - Code-Generierung
5. **Phi-3 Mini** - 2.36 GB - Microsoft
6. **Mistral 7B** - 4.37 GB - Vielseitig
7. **Kompakte Modelle** (1B-1.5B) - FÃ¼r schwache Hardware

### Features:
- âœ… Detaillierte Beschreibungen
- âœ… Ratings & Download-Zahlen
- âœ… Sprachen & Use-Cases
- âœ… RAM-Anforderungen
- âœ… GroÃŸes Download-Modal mit:
  - Echtzeit-Progress
  - Geschwindigkeit (MB/s)
  - GeschÃ¤tzte Zeit
  - Abbrechen-Funktion

---

## ğŸš€ Wie es funktioniert

### Beim Ã–ffnen des Model Managers:

1. **Provider wird geladen:**
   ```javascript
   GET /api/llm/providers/active
   â†’ { "provider": "llamacpp", "available": true }
   ```

2. **Modelle werden geladen:**
   ```javascript
   if (provider === 'ollama') {
     models = await api.getOllamaLibraryModels()
   } else if (provider === 'llamacpp') {
     models = await api.get('/model-store/all')
   }
   ```

3. **Anzeige passt sich an:**
   - **Banner**: "Ollama Library" vs "Model Store"
   - **Modell-Info**: Unterschiedliche Felder
   - **Download**: Unterschiedliche Dialoge

### Beim Download (llama.cpp):

1. **Modal Ã¶ffnet sich** mit Modell-Info
2. **EventSource** startet fÃ¼r SSE:
   ```javascript
   GET /api/model-store/download/{modelId}
   â†’ Stream: progress events
   ```
3. **Progress wird geparst:**
   - Prozent, GrÃ¶ÃŸe, Geschwindigkeit
   - Status-Log wird aktualisiert
4. **Nach Abschluss:**
   - Modal schlieÃŸt nach 2 Sekunden
   - Modelle werden neu geladen
   - Erscheint unter "Installierte Modelle"

---

## ğŸ“‚ Verzeichnisstruktur

```
models/
â”œâ”€â”€ library/          â† Heruntergeladene Modelle (Model Store)
â”‚   â””â”€â”€ qwen2.5-3b-instruct-q4_k_m.gguf
â””â”€â”€ custom/           â† Eigene hochgeladene Modelle
    â””â”€â”€ Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

---

## ğŸ¯ Erweiterbarkeit fÃ¼r neue Provider

Um einen neuen Provider hinzuzufÃ¼gen:

### Backend:
1. Neues `XxxProvider.java` implementieren
2. In `LLMProviderService` registrieren

### Frontend (ModelManager.vue):
```javascript
async function loadLibraryModels() {
  if (activeProvider.value === 'ollama') {
    // Ollama logic
  } else if (activeProvider.value === 'llamacpp') {
    // llama.cpp logic
  } else if (activeProvider.value === 'mein-neuer-provider') {
    // Neue Provider-Logik hier
    availableModels.value = await api.get('/mein-provider/models')
  }
}
```

### Template (ModelManager.vue):
```vue
<span v-if="activeProvider === 'ollama'">
  Ollama Info
</span>
<span v-else-if="activeProvider === 'llamacpp'">
  llama.cpp Info
</span>
<span v-else-if="activeProvider === 'mein-neuer-provider'">
  Neue Provider Info
</span>
```

---

## âœ… Testing

### Schritte:
1. **Starte Fleet Navigator**
   ```bash
   ./START.sh
   ```

2. **Wechsle Provider** in Einstellungen â†’ LLM Provider

3. **Ã–ffne Model Manager** (ğŸ§  Icon in TopBar)

4. **Tab "VerfÃ¼gbare Modelle":**
   - Bei **Ollama**: Sollte Ollama Library zeigen
   - Bei **llama.cpp**: Sollte Model Store mit 9 Modellen zeigen

5. **Teste Download** (llama.cpp):
   - Klicke "â¬‡ Download" bei einem Modell
   - GroÃŸes Modal sollte erscheinen
   - Progress sollte sichtbar sein

6. **Nach Download:**
   - Modell erscheint unter "Installierte Modelle"
   - Kann im Chat ausgewÃ¤hlt werden

---

## ğŸ“ Wichtige Dateien

### Backend:
- `LLMProviderController.java` - Provider API
- `ModelStoreController.java` - Model Store API
- `ModelRegistry.java` - 9 kuratierte Modelle
- `ModelDownloadService.java` - HuggingFace Download
- `LlamaCppProvider.java` - Provider mit neuer Verzeichnisstruktur

### Frontend:
- `ModelManager.vue` - Zentrale Modellverwaltung (erweitert)
- `ModelDownloadModal.vue` - GroÃŸes Download-Modal
- `SettingsModal.vue` - Model Store Tab entfernt
- `ModelStore.vue` - Standalone (optional, nicht mehr in Settings)

---

## ğŸ‰ Vorteile

âœ… **Eine zentrale Stelle** fÃ¼r alle Modelle
âœ… **Provider-unabhÃ¤ngig** - leicht erweiterbar
âœ… **Bessere UX** - User muss nicht wechseln
âœ… **Konsistente Bedienung** - gleiche UI fÃ¼r alle Provider
âœ… **Zukunftssicher** - neue Provider einfach hinzufÃ¼gen

---

**Erstellt:** 2025-11-11
**Version:** 0.2.9
**Autor:** JavaFleet Systems Consulting & Claude Code
