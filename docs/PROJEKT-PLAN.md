# Eigene LLM Web-Schnittstelle - Vollst√§ndige Planung

## Zusammenfassung

**Ziel:** Eigene Web-UI f√ºr Ollama mit voller Kontrolle √ºber System-Prompts und optimiert f√ºr produktive Code-Arbeit.

**Tech-Stack:** Spring Boot + Vue.js + SQLite + Ollama

**Entwicklungszeit:** 3-5 Tage f√ºr vollst√§ndige Version 1

---

## Verf√ºgbare Modelle & Context Windows

| Modell | Parameter | Context Window | Tokens | Zeichen (ca.) | Empfohlen f√ºr |
|--------|-----------|----------------|--------|---------------|---------------|
| **CodeLlama 70B** | 70B | 16.384 | 16k | ~65.000 | Komplexe Fragen, beste Qualit√§t |
| **Qwen2.5-Coder 7B** | 7B | 32.768 | 32k | ~130.000 | Viele Dateien, Code-Reviews |
| **Llama 3.2 3B** | 3B | 128.000 | 128k | ~512.000 | Riesige Kontexte, simple Fragen |

**Wichtig:** Gr√∂√üere Modelle ‚â† gr√∂√üerer Context! Neuere, kleinere Modelle haben oft mehr Context.

---

## Version 1 - Kern-Features (MVP)

### ‚úÖ Basis-Funktionalit√§t
1. **Chat-Interface**
   - Nachrichten senden/empfangen
   - Chat-History anzeigen
   - Neue Chats erstellen

2. **System-Prompt Management**
   - System-Prompt pro Chat konfigurierbar
   - Vorlagen (Templates) f√ºr h√§ufige Use Cases

3. **Multi-Modell-Support** ‚ö†Ô∏è WICHTIG
   - Dropdown zur Modell-Auswahl
   - Anzeige: Context-Gr√∂√üe, Parameter-Count
   - Wechsel zwischen Modellen m√∂glich

4. **SQLite Persistierung**
   - Chats speichern
   - Messages speichern
   - Global Stats (Token-Counter)

### ‚úÖ Kritische Features

5. **Streaming mit Toggle** ‚ö†Ô∏è PFLICHT
   - An/Aus schaltbar in Settings
   - Checkbox: "Streaming aktivieren"
   - Server-Sent Events (SSE) f√ºr Streaming
   - Fallback auf normale Requests

6. **Stop-Button** ‚ö†Ô∏è PFLICHT
   - Immer sichtbar w√§hrend Generierung
   - SSE Connection abbrechen
   - Feedback: "Generierung gestoppt"

7. **Systemlast-Monitor** ‚ö†Ô∏è PFLICHT
   - **Live-Anzeige** (Update alle 2 Sekunden)
   - CPU-Auslastung (%)
   - RAM-Nutzung (GB / Total GB)
   - GPU-Auslastung (% - falls vorhanden)
   - GPU-VRAM (GB / Total GB - falls vorhanden)
   - Sidebar oder Header-Bereich

8. **Token-Counter** ‚ö†Ô∏è PFLICHT
   - **Pro Chat**: Aktuelle Token-Count mit Progress-Bar
   - **Warnung bei 80%** des Context-Limits
   - **Gesamt-Statistik**: Alle generierten Tokens
   - **Reset-Funktion** f√ºr Gesamt-Statistik

### ‚úÖ Essentials f√ºr Code-Arbeit

9. **Markdown-Rendering** ‚ö†Ô∏è PFLICHT
   - `marked.js` f√ºr Markdown-Parsing
   - √úberschriften (#, ##)
   - Fett/Kursiv (**, *)
   - Listen, Links

10. **Code-Highlighting** ‚ö†Ô∏è PFLICHT
    - `highlight.js` f√ºr Syntax-Highlighting
    - Auto-Detection der Sprache
    - Unterst√ºtzung: Java, JavaScript, Python, etc.
    - Copy-Button f√ºr Code-Bl√∂cke

### ‚úÖ Context-Management (KRITISCH!)

11. **File Upload**
    - Dateien hochladen und in Context laden
    - Unterst√ºtzte Formate: .java, .js, .py, .xml, .json, .txt
    - Max. Gr√∂√üe: Warnung bei Context-Limit

12. **Context-Viewer**
    - Liste aller Items im Context:
      - System-Prompt
      - Hochgeladene Dateien
      - Chat-History
    - Token-Count pro Item
    - Gesamt-Token-Anzeige mit Progress-Bar

13. **Context-Management**
    - Manuelle Entfernung einzelner Items
    - Auto-Sliding-Window (alte Messages entfernen)
    - Warnung: "Context-Limit erreicht"

14. **Smart Model Recommendation** ‚ö†Ô∏è WICHTIG
    - Analysiert Context-Gr√∂√üe
    - Empfiehlt passendes Modell
    - Beispiel: "18k Tokens ‚Üí Nutze Qwen2.5 statt CodeLlama"

---

## Datenbank-Schema (SQLite)

### Entities

```java
@Entity
public class Chat {
    @Id
    @GeneratedValue
    private Long id;
    private String title;
    private String systemPrompt;
    private String modelName;
    private LocalDateTime createdAt;
    private LocalDateTime updatedAt;

    @OneToMany(mappedBy = "chat", cascade = CascadeType.ALL)
    private List<Message> messages;

    @OneToMany(mappedBy = "chat", cascade = CascadeType.ALL)
    private List<ContextItem> contextItems;
}

@Entity
public class Message {
    @Id
    @GeneratedValue
    private Long id;
    @ManyToOne
    private Chat chat;
    private String role; // "system", "user", "assistant"
    @Lob
    private String content;
    private Integer tokenCount;
    private LocalDateTime timestamp;
}

@Entity
public class ContextItem {
    @Id
    @GeneratedValue
    private Long id;
    @ManyToOne
    private Chat chat;
    private String type; // "file", "text"
    private String name; // Filename oder "Manual Text"
    @Lob
    private String content;
    private Integer tokenCount;
    private LocalDateTime addedAt;
}

@Entity
public class GlobalStats {
    @Id
    private Long id = 1L; // Singleton
    private Long totalTokensGenerated = 0L;
    private Long totalTokensInput = 0L;
    private Long totalChats = 0L;
    private Long totalMessages = 0L;
    private LocalDateTime lastReset;
}
```

---

## Backend API Endpoints

```
# Modelle
GET    /api/models                      # Liste aller Ollama-Modelle mit Context-Info

# Chats
GET    /api/chats                       # Alle Chats
POST   /api/chats                       # Neuen Chat erstellen
GET    /api/chats/{id}                  # Chat laden
PUT    /api/chats/{id}                  # Chat aktualisieren (Titel, System-Prompt)
DELETE /api/chats/{id}                  # Chat l√∂schen

# Messages
POST   /api/chats/{id}/message          # Message senden (non-streaming)
GET    /api/chats/{id}/stream           # SSE Stream f√ºr Messages
POST   /api/chats/{id}/stop             # Generierung abbrechen

# Context-Management
POST   /api/chats/{id}/context/file     # Datei hochladen
POST   /api/chats/{id}/context/text     # Text manuell hinzuf√ºgen
GET    /api/chats/{id}/context          # Alle Context-Items
DELETE /api/chats/{id}/context/{itemId} # Context-Item entfernen
GET    /api/chats/{id}/context/tokens   # Token-Count des Context

# System-Monitoring
GET    /api/system/metrics              # CPU, RAM, GPU, VRAM (Live)

# Statistiken
GET    /api/stats/global                # Gesamt-Token-Counter
POST   /api/stats/reset                 # Reset Global Stats

# Export
GET    /api/chats/{id}/export/markdown  # Chat als Markdown
GET    /api/chats/{id}/export/json      # Chat als JSON
```

---

## Systemlast-Monitor (Backend)

```java
@RestController
@RequestMapping("/api/system")
public class SystemMetricsController {

    @GetMapping("/metrics")
    public SystemMetrics getMetrics() {
        OperatingSystemMXBean osBean =
            (OperatingSystemMXBean) ManagementFactory.getOperatingSystemMXBean();

        Runtime runtime = Runtime.getRuntime();

        return SystemMetrics.builder()
            .cpuUsage(osBean.getSystemCpuLoad() * 100)
            .ramUsedGB((runtime.totalMemory() - runtime.freeMemory()) / 1024.0 / 1024.0 / 1024.0)
            .ramTotalGB(runtime.maxMemory() / 1024.0 / 1024.0 / 1024.0)
            .gpuUsage(getGPUUsage())
            .gpuVramUsedGB(getGPUVRAM())
            .gpuVramTotalGB(getGPUVRAMTotal())
            .build();
    }

    private Double getGPUUsage() {
        try {
            Process process = Runtime.getRuntime()
                .exec("nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits");

            BufferedReader reader = new BufferedReader(
                new InputStreamReader(process.getInputStream()));
            String line = reader.readLine();

            return line != null ? Double.parseDouble(line.trim()) : null;
        } catch (Exception e) {
            return null; // Keine GPU oder nvidia-smi nicht verf√ºgbar
        }
    }

    private Double getGPUVRAM() {
        try {
            Process process = Runtime.getRuntime()
                .exec("nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits");

            BufferedReader reader = new BufferedReader(
                new InputStreamReader(process.getInputStream()));
            String line = reader.readLine();

            return line != null ? Double.parseDouble(line.trim()) / 1024.0 : null;
        } catch (Exception e) {
            return null;
        }
    }

    private Double getGPUVRAMTotal() {
        try {
            Process process = Runtime.getRuntime()
                .exec("nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits");

            BufferedReader reader = new BufferedReader(
                new InputStreamReader(process.getInputStream()));
            String line = reader.readLine();

            return line != null ? Double.parseDouble(line.trim()) / 1024.0 : null;
        } catch (Exception e) {
            return null;
        }
    }
}
```

---

## Token-Counter (Backend)

```java
@Service
public class TokenCounterService {

    /**
     * Approximation: 1 Token ‚âà 4 Zeichen (Englisch), ‚âà 3-4 Zeichen (Deutsch)
     * F√ºr genauere Z√§hlung: tiktoken-java oder sentencepiece
     */
    public int countTokens(String text) {
        return (int) Math.ceil(text.length() / 4.0);
    }

    public int getTotalChatTokens(Chat chat) {
        int total = 0;

        // System-Prompt
        if (chat.getSystemPrompt() != null) {
            total += countTokens(chat.getSystemPrompt());
        }

        // Messages
        for (Message msg : chat.getMessages()) {
            total += msg.getTokenCount();
        }

        // Context-Items
        for (ContextItem item : chat.getContextItems()) {
            total += item.getTokenCount();
        }

        return total;
    }

    public boolean isContextLimitReached(Chat chat, String modelName) {
        int contextLimit = getModelContextLimit(modelName);
        int currentTokens = getTotalChatTokens(chat);

        return currentTokens > (contextLimit * 0.8); // 80% Warnung
    }

    private int getModelContextLimit(String modelName) {
        if (modelName.contains("codellama:70b")) return 16384;
        if (modelName.contains("qwen2.5-coder")) return 32768;
        if (modelName.contains("llama3.2")) return 128000;
        return 4096; // Default fallback
    }
}
```

---

## Smart Model Recommendation

```java
@Service
public class ModelRecommendationService {

    @Autowired
    private TokenCounterService tokenCounter;

    public ModelRecommendation recommendModel(Chat chat, List<OllamaModel> availableModels) {
        int contextTokens = tokenCounter.getTotalChatTokens(chat);
        String currentModel = chat.getModelName();

        // Aktuelles Modell passt noch
        int currentLimit = tokenCounter.getModelContextLimit(currentModel);
        if (contextTokens < currentLimit * 0.7) {
            return ModelRecommendation.builder()
                .currentModelOk(true)
                .currentModel(currentModel)
                .build();
        }

        // Suche besseres Modell
        for (OllamaModel model : availableModels) {
            int limit = tokenCounter.getModelContextLimit(model.getName());

            if (contextTokens < limit * 0.7) {
                return ModelRecommendation.builder()
                    .currentModelOk(false)
                    .recommendedModel(model.getName())
                    .reason("Dein Context (" + contextTokens + " Tokens) √ºberschreitet "
                           + currentModel + " Limit (" + currentLimit + " Tokens)")
                    .build();
            }
        }

        // Kein Modell passt - Context reduzieren
        return ModelRecommendation.builder()
            .currentModelOk(false)
            .warningTooLarge(true)
            .message("Context zu gro√ü f√ºr alle Modelle. Bitte Dateien/Messages entfernen.")
            .build();
    }
}
```

---

## Context-Management (Backend)

```java
@Service
public class ContextManagementService {

    @Autowired
    private ContextItemRepository contextRepo;

    @Autowired
    private TokenCounterService tokenCounter;

    public ContextItem addFile(Long chatId, MultipartFile file) throws Exception {
        String content = new String(file.getBytes(), StandardCharsets.UTF_8);
        int tokens = tokenCounter.countTokens(content);

        // Context-Limit pr√ºfen
        Chat chat = chatService.getChat(chatId);
        int currentTokens = tokenCounter.getTotalChatTokens(chat);
        int limit = tokenCounter.getModelContextLimit(chat.getModelName());

        if (currentTokens + tokens > limit * 0.9) {
            throw new ContextLimitException(
                "Datei zu gro√ü! W√ºrde Context-Limit √ºberschreiten.");
        }

        ContextItem item = new ContextItem();
        item.setChat(chat);
        item.setType("file");
        item.setName(file.getOriginalFilename());
        item.setContent(content);
        item.setTokenCount(tokens);
        item.setAddedAt(LocalDateTime.now());

        return contextRepo.save(item);
    }

    public List<Message> prepareMessagesWithSlidingWindow(Chat chat) {
        List<Message> messages = chat.getMessages();
        int maxTokens = tokenCounter.getModelContextLimit(chat.getModelName());

        // System-Prompt
        int usedTokens = tokenCounter.countTokens(chat.getSystemPrompt());

        // Context-Items (Dateien)
        for (ContextItem item : chat.getContextItems()) {
            usedTokens += item.getTokenCount();
        }

        // Reserve f√ºr Antwort
        int availableForMessages = maxTokens - usedTokens - 2000;

        // Messages von hinten nach vorne (neueste zuerst)
        List<Message> result = new ArrayList<>();
        for (int i = messages.size() - 1; i >= 0; i--) {
            Message msg = messages.get(i);

            if (usedTokens + msg.getTokenCount() > availableForMessages) {
                break; // Zu alt, nicht mehr einbeziehen
            }

            result.add(0, msg);
            usedTokens += msg.getTokenCount();
        }

        return result;
    }
}
```

---

## System-Prompt Vorschl√§ge

### Template 1: Deutscher Code-Assistent (Empfohlen)
```
Du bist ein erfahrener deutscher Software-Entwickler und Code-Assistent.

SPRACHE: Antworte IMMER auf Deutsch, egal in welcher Sprache du gefragt wirst.

FORMATIERUNG:
- Nutze Markdown f√ºr Struktur
- # f√ºr Haupt√ºberschriften, ## f√ºr Unter√ºberschriften
- ** f√ºr wichtige Begriffe
- ` f√ºr inline Code
- ``` f√ºr Code-Bl√∂cke mit Sprach-Tag (```java, ```javascript, ```python)
- KEINE Emojis verwenden

STIL:
- Pr√§zise und professionell
- Erkl√§re Code verst√§ndlich
- Best Practices beachten
- Sicherheitsprobleme ansprechen (SQL-Injection, XSS, etc.)
- Bei Code-Reviews: Konstruktives Feedback

REGELN:
- Wenn du Code schreibst: F√ºge Kommentare hinzu
- Wenn du unsicher bist: Sage es
- Keine halluzinierten Bibliotheken oder APIs
```

### Template 2: Code-Reviewer
```
Du bist ein erfahrener Code-Reviewer.

Deine Aufgabe:
- Code auf Bugs analysieren
- Performance-Probleme identifizieren
- Sicherheitsl√ºcken finden (OWASP Top 10)
- Best Practices vorschlagen
- Code-Smell erkennen

Format:
- Nutze Markdown mit √úberschriften
- Strukturiere nach: Bugs, Security, Performance, Style
- Zeige problematischen Code mit ```
- Gib Verbesserungsvorschl√§ge

Sprache: Deutsch
Keine Emojis.
```

### Template 3: Architektur-Berater
```
Du bist ein Software-Architektur-Experte.

Fokus:
- Design Patterns (SOLID, Gang of Four)
- Microservices vs. Monolith
- Datenbank-Design
- API-Design (REST, GraphQL)
- Skalierbarkeit

Stil:
- Erkl√§re Trade-offs
- Zeige Vor- und Nachteile
- Nutze Diagramme (ASCII-Art oder Mermaid)
- Markdown-Formatierung

Sprache: Deutsch
Keine Emojis.
```

---

## UI-Layout (3-Spalten-Design)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üî∑ LLM WebUI          [Modell: CodeLlama 70B ‚ñº] [Context: 12k/16k]  [‚öôÔ∏è]   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          ‚îÇ                                              ‚îÇ  üìä SYSTEM         ‚îÇ
‚îÇ üí¨ Chats ‚îÇ  Chat: "Spring Boot Projekt"                ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  CPU: 45%          ‚îÇ
‚îÇ + Neu    ‚îÇ                                              ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚îÇ
‚îÇ          ‚îÇ  üë§ User:                                    ‚îÇ                    ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ  Erkl√§re Dependency Injection                ‚îÇ  RAM: 12/32 GB     ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚îÇ
‚îÇ Sprint 1 ‚îÇ  ü§ñ Assistant:                               ‚îÇ                    ‚îÇ
‚îÇ Code Rev ‚îÇ  # Dependency Injection                      ‚îÇ  GPU: 98%          ‚îÇ
‚îÇ Archit.. ‚îÇ                                              ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë        ‚îÇ
‚îÇ          ‚îÇ  **Dependency Injection** ist ein Design-    ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  Pattern zur Entkopplung...                  ‚îÇ  VRAM: 18/24 GB    ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë        ‚îÇ
‚îÇ          ‚îÇ  ```java                                     ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  @Autowired                                  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ          ‚îÇ  private MyService service;                  ‚îÇ  üìà TOKENS         ‚îÇ
‚îÇ          ‚îÇ  ```                                         ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  [streaming...]                              ‚îÇ  Chat:             ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ  12.456 / 16.384   ‚îÇ
‚îÇ          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë       ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ  76% ‚ö†Ô∏è            ‚îÇ
‚îÇ          ‚îÇ  üìö Context (4 Items):                       ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  ‚Ä¢ System-Prompt (200 T)                     ‚îÇ  Gesamt:           ‚îÇ
‚îÇ          ‚îÇ  ‚Ä¢ UserService.java (1.2k T) [‚úñ]            ‚îÇ  1.234.567         ‚îÇ
‚îÇ          ‚îÇ  ‚Ä¢ Config.xml (456 T) [‚úñ]                    ‚îÇ  42 Chats          ‚îÇ
‚îÇ          ‚îÇ  ‚Ä¢ Chat History (8.7k T)                     ‚îÇ  [üîÑ Reset]        ‚îÇ
‚îÇ          ‚îÇ  [üìé Datei] [üìù Text]                        ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
‚îÇ          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  ‚öôÔ∏è SETTINGS       ‚îÇ
‚îÇ          ‚îÇ  üí° Empfehlung: Context bei 76%!            ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  Nutze Qwen2.5 (32k) f√ºr mehr Platz?       ‚îÇ  Streaming:        ‚îÇ
‚îÇ          ‚îÇ  [Wechseln] [Ignorieren]                     ‚îÇ  ‚òë Aktiviert      ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  System-Prompt: [Code-Assistent ‚ñº]          ‚îÇ  Theme:            ‚îÇ
‚îÇ          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  üåô Dunkel        ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ                    ‚îÇ
‚îÇ          ‚îÇ  [Deine Frage...]                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ          ‚îÇ
‚îÇ          ‚îÇ  [üõë Stop] [üì§ Senden]
‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Bereiche:

#### Linke Sidebar (Chat-Liste)
- Button: Neuer Chat
- Liste aller Chats (scrollbar)
- Aktiver Chat hervorgehoben
- Hover: Delete-Button

#### Mittlerer Bereich (Haupt-Chat)
- Chat-Titel (editierbar)
- Nachrichtenverlauf
  - User-Messages (rechts, blau)
  - Assistant-Messages (links, grau)
  - Markdown-gerendert
  - Code mit Syntax-Highlighting
- **Context-Viewer** (aufklappbar)
  - Liste aller Context-Items
  - Token-Count pro Item
  - X-Button zum Entfernen
- **Smart Recommendation** (wenn n√∂tig)
- System-Prompt Dropdown
- Eingabefeld mit Auto-Resize
- Stop/Senden Buttons

#### Rechte Sidebar (Monitoring & Stats)
- **System-Monitor** (Live, alle 2s)
  - CPU Progress-Bar
  - RAM Progress-Bar
  - GPU Progress-Bar (falls vorhanden)
  - VRAM Progress-Bar (falls vorhanden)
- **Token-Statistiken**
  - Chat-Token mit Progress-Bar
  - Warnung bei >80%
  - Gesamt-Token-Counter
  - Reset-Button
- **Settings** (kompakt)
  - Streaming Toggle
  - Theme Switch

---

## Technologie-Stack

### Backend
- **Spring Boot 3.x** (Java 17+)
- **Spring WebFlux** (f√ºr Streaming)
- **Spring Data JPA** (f√ºr SQLite)
- **SQLite** (embedded DB)
- **RestTemplate/WebClient** (Ollama API)
- **Lombok** (Boilerplate reduzieren)
- **Spring Boot Actuator** (System-Metriken)

### Frontend
- **Vue.js 3** (Composition API)
- **Vite** (Build-Tool, schnell)
- **Axios** (HTTP Client)
- **EventSource** (f√ºr SSE/Streaming)
- **Tailwind CSS** (Utility-First CSS)
- **marked.js** (Markdown ‚Üí HTML)
- **highlight.js** (Code Syntax-Highlighting)
- **Chart.js** (optional: f√ºr Token-Statistik-Diagramme)

### DevOps
- **Maven** (Build)
- **H2** (f√ºr Tests, statt SQLite)
- **JUnit 5** + **Mockito** (Testing)

---

## Aufwand-Sch√§tzung (Finale Version)

### Version 1 - MVP mit allen Kern-Features

**Features:**
- ‚úÖ Basis-Chat (senden/empfangen)
- ‚úÖ System-Prompt Management
- ‚úÖ Multi-Modell-Support
- ‚úÖ Streaming mit Toggle
- ‚úÖ Stop-Button
- ‚úÖ Systemlast-Monitor (CPU, RAM, GPU, VRAM)
- ‚úÖ Token-Counter (pro Chat + gesamt)
- ‚úÖ Markdown-Rendering
- ‚úÖ Code-Highlighting
- ‚úÖ File Upload
- ‚úÖ Context-Viewer
- ‚úÖ Context-Management (Entfernen, Sliding-Window)
- ‚úÖ Smart Model Recommendation
- ‚úÖ SQLite Persistierung

**Aufwand:**

| Komponente | Stunden | Details |
|------------|---------|---------|
| **Backend** | **16-20h** | |
| Projekt-Setup | 1h | Spring Initializr, Dependencies |
| Ollama API Client | 2h | REST Client mit Streaming |
| JPA Entities & Repos | 2h | Chat, Message, ContextItem, Stats |
| Chat Service | 2h | CRUD, Message-Handling |
| Context Management | 3h | File Upload, Token-Counter, Sliding-Window |
| System-Monitoring | 2h | CPU/RAM/GPU-Auslastung |
| Smart Model Recommendation | 2h | Logik + API |
| API Controllers | 2h | REST Endpoints |
| | | |
| **Frontend** | **18-22h** | |
| Vue.js Setup | 1h | Vite, Router, State Management |
| Chat-UI (Basis) | 4h | Layout, Message-Liste, Input |
| Streaming-Integration | 2h | EventSource, Stop-Button |
| Sidebar (Chats) | 2h | Chat-Liste, Neu-Button |
| Sidebar (Monitoring) | 3h | Live-Metriken, Token-Stats |
| Markdown + Highlighting | 2h | marked.js + highlight.js Integration |
| File Upload UI | 2h | Drag&Drop, Progress |
| Context-Viewer | 2h | Liste, Entfernen-Funktion |
| Model Recommendation UI | 1h | Modal/Toast mit Empfehlung |
| Settings-Dialog | 2h | Streaming Toggle, Theme |
| | | |
| **Testing & Polish** | **6-8h** | |
| Backend-Tests | 2h | Unit-Tests f√ºr Services |
| Frontend-Tests | 2h | Component-Tests |
| Integration-Testing | 2h | End-to-End Szenarien |
| Bug-Fixing | 2h | |
| | | |
| **GESAMT** | **40-50h** | **‚âà 5-6 Arbeitstage** |

**Realistisch:** Bei 8h/Tag konzentrierter Arbeit: **5-7 Tage**

---

## Entwicklungs-Roadmap

### Phase 1: Foundation (Tag 1-2)
1. ‚úÖ Spring Boot Projekt aufsetzen
2. ‚úÖ SQLite + JPA Entities (Chat, Message, ContextItem, GlobalStats)
3. ‚úÖ Ollama REST Client (Basic, ohne Streaming)
4. ‚úÖ Vue.js Projekt + Layout (3-Spalten)
5. ‚úÖ Chat CRUD (erstellen, laden, l√∂schen)
6. ‚úÖ Messages senden/empfangen (non-streaming erst)
7. ‚úÖ Modell-Auswahl Dropdown

### Phase 2: Kern-Features (Tag 3-4)
1. ‚úÖ **Streaming** mit SSE implementieren
2. ‚úÖ **Stop-Button** (SSE Abbruch)
3. ‚úÖ **Token-Counter** Backend + Frontend
4. ‚úÖ **Systemlast-Monitor** (nvidia-smi Integration)
5. ‚úÖ **Live-Updates** f√ºr Monitoring (Polling alle 2s)
6. ‚úÖ **Markdown-Rendering** (marked.js)
7. ‚úÖ **Code-Highlighting** (highlight.js)
8. ‚úÖ **Streaming Toggle** in Settings

### Phase 3: Context-Management (Tag 4-5)
1. ‚úÖ **File Upload** Backend + Frontend
2. ‚úÖ **Context-Viewer** (Liste aller Items)
3. ‚úÖ **Context-Item Entfernung**
4. ‚úÖ **Sliding-Window** Implementierung
5. ‚úÖ **Smart Model Recommendation**
6. ‚úÖ **Warnung bei Context-Limit**

### Phase 4: Polish & Testing (Tag 5-6)
1. ‚úÖ Error-Handling (Backend + Frontend)
2. ‚úÖ Loading-States & Spinners
3. ‚úÖ Responsive Design (Mobile-Ansicht)
4. ‚úÖ Dark Theme (optional)
5. ‚úÖ Keyboard-Shortcuts (Enter = Senden, Strg+L = Neuer Chat)
6. ‚úÖ Unit-Tests
7. ‚úÖ Integration-Tests
8. ‚úÖ Bug-Fixing

### Phase 5: Optional Erweiterungen (Tag 6+)
1. ‚≠ê Export/Import (JSON, Markdown)
2. ‚≠ê Parameter-Sliders (Temperature, Top-P, etc.)
3. ‚≠ê Message-Pinning (wichtige Messages behalten)
4. ‚≠ê RAG-System (f√ºr riesige Codebasen)
5. ‚≠ê Auto-Summarization (alte Chats zusammenfassen)
6. ‚≠ê Multi-User Support (Login/Auth)

---

## Ollama API Referenz

### Liste aller Modelle
```bash
curl http://localhost:11434/api/tags
```

**Response:**
```json
{
  "models": [
    {
      "name": "codellama:70b",
      "size": 38818143488,
      "digest": "...",
      "details": {
        "format": "gguf",
        "family": "llama",
        "parameter_size": "70B"
      }
    }
  ]
}
```

### Chat (non-streaming)
```bash
curl http://localhost:11434/api/chat -d '{
  "model": "codellama:70b",
  "messages": [
    {"role": "system", "content": "Du bist ein deutscher Code-Assistent"},
    {"role": "user", "content": "Erkl√§re Spring Boot"}
  ],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "top_p": 0.9
  }
}'
```

**Response:**
```json
{
  "model": "codellama:70b",
  "message": {
    "role": "assistant",
    "content": "Spring Boot ist ein Framework..."
  },
  "done": true,
  "total_duration": 12345678900,
  "prompt_eval_count": 50,
  "eval_count": 234
}
```

### Chat (streaming)
```bash
curl http://localhost:11434/api/chat -d '{
  "model": "codellama:70b",
  "messages": [
    {"role": "system", "content": "Du bist ein Code-Assistent"},
    {"role": "user", "content": "Hallo"}
  ],
  "stream": true
}'
```

**Response (NDJSON - eine Zeile pro Token):**
```json
{"message":{"role":"assistant","content":"Hallo"},"done":false}
{"message":{"role":"assistant","content":"!"},"done":false}
{"message":{"role":"assistant","content":" Wie"},"done":false}
{"message":{"role":"assistant","content":" kann"},"done":false}
{"message":{"role":"assistant","content":" ich"},"done":false}
{"message":{"role":"assistant","content":" helfen"},"done":false}
{"message":{"role":"assistant","content":"?"},"done":false}
{"done":true,"total_duration":123456789,"eval_count":7}
```

---

## Vorteile gegen√ºber Open WebUI

‚úÖ **Volle Kontrolle** √ºber System-Prompts (keine Bugs!)
‚úÖ **Keine Blob-Inkompatibilit√§t** - direkter Ollama-Zugriff
‚úÖ **Ma√ügeschneidert** f√ºr deine Bed√ºrfnisse
‚úÖ **Transparenz** - du verstehst jeden Teil
‚úÖ **Erweiterbar** - Features nach Bedarf
‚úÖ **Kein Tool-Lock-In** - nutzt Standard Ollama API
‚úÖ **Lerneffekt** - du baust es selbst
‚úÖ **Performance** - optimiert f√ºr deine Hardware
‚úÖ **Context-Management** - besser als Open WebUI
‚úÖ **Multi-Modell** - smart switching basierend auf Context

---

## N√§chste Schritte

### 1. Entscheidung treffen
- [ ] MVP jetzt bauen?
- [ ] Features priorisieren?
- [ ] Andere Tools erst testen?

### 2. Projekt aufsetzen
```bash
# Backend
spring init --dependencies=web,data-jpa,lombok \
  --groupId=com.myapp --artifactId=llm-webui \
  --name=llm-webui backend

# Frontend
npm create vue@latest frontend
cd frontend
npm install
npm install axios marked highlight.js
```

### 3. Erste Schritte
1. Ollama API testen (curl)
2. Backend: Ollama Client implementieren
3. Frontend: Basis-Layout
4. Erste Message senden/empfangen

---

## Ressourcen

- **Ollama API Docs:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **Spring WebFlux SSE:** https://www.baeldung.com/spring-server-sent-events
- **Vue.js EventSource:** https://developer.mozilla.org/en-US/docs/Web/API/EventSource
- **SQLite + Spring Boot:** https://www.baeldung.com/spring-boot-sqlite
- **marked.js:** https://marked.js.org/
- **highlight.js:** https://highlightjs.org/
- **Tailwind CSS:** https://tailwindcss.com/
- **nvidia-smi Cheatsheet:** https://nvidia.custhelp.com/app/answers/detail/a_id/3751

---

## FAQ

### Warum Spring Boot und nicht Node.js?
- Du kennst Java/Spring Boot bereits
- Bessere Typ-Sicherheit
- JPA f√ºr Datenbank einfacher
- Performance bei CPU-intensiven Tasks

### Warum Vue.js und nicht React?
- Einfacher f√ºr Einsteiger
- Weniger Boilerplate
- Gute Dokumentation
- Beide funktionieren - w√§hle was du kennst!

### Kann ich andere LLM-Backends nutzen?
Ja! Die Architektur ist flexibel:
- Ollama (aktuell)
- LM Studio
- OpenAI API
- LocalAI
- Jedes REST-API-basierte Backend

### Was ist mit Docker?
Sp√§ter kannst du Dockerize:
```dockerfile
FROM eclipse-temurin:17-jre
COPY target/llm-webui.jar app.jar
ENTRYPOINT ["java", "-jar", "app.jar"]
```

---

**üéØ Bist du bereit zu starten? Sag mir, wenn du mit dem Bau beginnen willst!**
